\documentclass{article}

% ready for submission
\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amssymb}
\usepackage{amsmath}}

\newcommand{\gsim}{\gtrsim}
\newcommand{\lsim}{\lesssim}



\title{An information-theoretic upper bound on prime gaps}

\date{September 1, 2021}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Aidan Rocke\\
  \texttt{aidanrocke@gmail.com} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
Within the setting of rare event modelling, the method of level sets allows 
us to define an equivalence relation over rare events with distinct rates of entropy production. As a measure of the efficacy of this method it is applied to Cram√©r's conjecture, an open problem in probabilistic number theory. Furthermore, this analysis places strong epistemic limits on the
application of machine learning to analyse the distribution of primes.
\end{abstract}

\section{Classifying rare events using level sets}
 
Sequences of rare events that are not finite-state compressible are recurrent, of variable frequency, and unpredictable relative to a finite-state machine such as a machine learning model. For these reasons, they are of great scientific interest. But, how might we classify rare events that satisfy these criteria? 

If we identify rare events using the function $1_X: \mathbb{N} \rightarrow \{0,1\}$, we may analyse the rate of
entropy production of the computable binary sequence $X_n = \{x_i\}_{i=1}^n \in \{0,1\}^n$. In order to qualify as a rare event, a randomly sampled element $x_z$ of the sequence $X_n$ must generally
pass $\pi_x(n)$ tests. These tests $A_i, A_{j \neq i} \in \{A_i\}_{i=1}^{\pi_x(n)}$ are de-correlated as rare events are assumed to be independent of each other: 

\begin{equation}
\forall z \sim U([1,n]), P(z \in A_i \land z \in A_{j \neq i}) = P(z \in A_i ) \cdot P(z \in A_{j \neq i})
\end{equation}

where the $P(z \in A_i )$ have a natural frequentist interpretation. We generally assume that the tests are consistent so $\{A_i\}_{i=1}^{\pi_x(n)} \subset \{A_i\}_{i=1}^{\pi_x(n+1)}$ where $\pi_{x}(n)$ counts the number of rare events in the time interval $[1,n]$ so it is monotonically increasing. Given (1), we may define the average \textit{probability density} using the inclusion-exclusion principle:

\begin{equation}
\forall z \sim U([1,n]), P(x_z =1) = P(z \in \bigcap_{k \leq \pi_x(n)} A_k) = \prod_{k=1}^{\pi_x(n)} P(z \in A_k)
\end{equation}

as well as the entropy of the uniformly distributed random variable:

\begin{equation}
\forall z \sim U([1,n]), H(X_n) = -\sum_{\lambda =1}^n \frac{1}{n} \cdot \ln P(x_z =1) = -\ln \prod_{k=1}^{\pi_x(n)} P(z \in A_k)
\end{equation}

which is a measure of expected surprise, or the expected information gained from observing a rare event.

Having defined the entropy (3), we may compare the entropies of distinct sequences using the typical probabilities
$q_n \in (0,1)$, which allow us to define an equivalence relation over entropies:

\begin{equation}
\mathcal{L}_{q_n} = \{X_n \in \{0,1\}^{\infty}: \ln \big(\frac{1}{q_n}\big) \sim H(X_n) \}
\end{equation}

\newpage 

Finally, (5) implies that:

\begin{equation}
\forall X_n \in \mathcal{L}_{q_n}, \lim_{n \to \infty} -\frac{\ln q_n}{H(X_n)} = 1
\end{equation}

so the entropy rates of distinct sequences $X_n,X'_n \in \mathcal{L}_{q_n}$ are
asymptotically equivalent. From a data compression perspective, this tells us 
that in the asymptotic limit we need $\sim \ln \big(\frac{1}{q_n}\big)$ bits in order to compress a particular class of rare events. 

One more interesting consequence is that the following holds \textit{asymptotically almost surely}: 

\begin{equation}
\frac{\partial}{\partial q_n} H(X_n) = \frac{\partial}{\partial q_n} -\ln q_n \implies \forall z \sim U([1,N]), P(x_z = 1) \sim q_N
\end{equation} 

and therefore for large $N \in \mathbb{N}$:

\begin{equation}
\pi_x(N) \sim N \cdot q_N 	
\end{equation} 

\subsection{Statistical generalisations of the Prime Number Theorem}

From statistical considerations, Gauss and Legendre inferred that the density of primes at $n \in \mathbb{N}$ is on the order of $\sim \frac{1}{\ln n}$. This led them to conjecture that the number of primes less than $N$ is on the order of:

\begin{equation}
\pi(N) \sim \int_{2}^N \frac{1}{\ln x} dx \sim N \cdot \frac{1}{\ln N}
\end{equation}

which is equivalent to the Prime Number Theorem.

However, given our assumptions we are more generally interested in the family of density functions that satisfy:  

\begin{equation}
0 < c \ll N, \int_{c}^N \frac{1}{f(x)} dx \sim N \cdot \frac{1}{f(N)}
\end{equation}

where $f$ is analytic and $\lim\limits_{N \to \infty} \frac{1}{f(N)}=0$. Using integration by parts, this is equivalent to the criterion:

\begin{equation}
\int_{c}^N g(x) dx = N \cdot g(N) - c \cdot g(c) - \int_{c}^N x \cdot g'(x) dx \sim N \cdot g(N)
\end{equation}

where $g(x)=\frac{1}{f(x)}$, and $g(x)$ dominates $x \cdot g'(x)$.
So this family naturally includes density functions of the form:

\begin{equation}
\exists a > 0 \forall x \in \mathbb{R}_+, g(x) = (\ln x)^{-a}
\end{equation}

which we may call the log-zeta distribution in the sense that the un-normalised zeta distribution
is given by:

\begin{equation}
\forall s >1, \zeta(s) = \zeta_s \circ \mathbb{N} = \sum_{n \in \mathbb{N}} \frac{1}{n^s}
\end{equation}

and if we log-transform the state-space $\mathbb{N}$ we have:

\begin{equation}
\forall s >1, \zeta_s \circ \ln \big(\mathbb{N}\big) = \sum_{n \in \mathbb{N}} \frac{1}{(\ln n)^s}
\end{equation}

where the logarithm transports us from the space of integers to the space of bits. It is worth adding that \textit{prime encodings} are of great interest to rare-event modellers as they are not finite-state compressible. This follows from the fact that the prime numbers are empirically-distributed as if they were arranged uniformly, an immediate consequence of the information-theoretic derivation of the Prime Number Theorem [9]. 

\subsection{The typical probability as the average probability}

Given that the density $g(x) = (\ln x)^{-a}$ is Riemann-Integrable on $[2,N]$ for $N < \infty$, by the Intermediate Value Theorem there exists a sequence $x_n \in [n,n+1]$ such that: 

\begin{equation}
\int_{2}^N (\ln x)^{-a} dx = \sum_{n=2}^N \frac{1}{(\ln x_n)^a}
\end{equation}

and since $\forall n \in \mathbb{N}, 1 \leq \big(\frac{\ln x_n}{\ln n}\big)^a \leq \big(\frac{\ln (n+1)}{\ln n}\big)^a$ where $\forall a > 0, \lim\limits_{n \to \infty} \big(\frac{\ln (n+1)}{\ln n}\big)^a = 1$ so we have: 

\begin{equation}
\int_{2}^N (\ln x)^{-a} dx \sim \sum_{n=2}^N \frac{1}{(\ln n)^a} \sim N \cdot \frac{1}{(\ln N)^a}
\end{equation}

and therefore the typical probability is asymptotically equivalent to the 
average probability: 

\begin{equation}
q_N \sim \frac{1}{N} \sum_{n=2}^N \frac{1}{(\ln n)^a} 	
\end{equation}

In fact, for large $n$ the average probability density in (2) may be expressed as: 

\begin{equation}
\forall z \sim U([1,n]), P(x_z = 1) \sim \frac{1}{n} \sum_{k=1}^n P(x_k = 1)	\end{equation}

since $P(x_{n+1}=1) \approx (n+1) \cdot q_{n+1} - n \cdot q_n = q_{n+1} + n \cdot (q_{n+1}-q_n) \sim q_{n+1}$. 

\subsection{The advantage of using the exponential of entropy}

It is worth noting that the \textit{entropy rate} $\frac{1}{q_n}$ corresponds 
to the exponential of entropy

\begin{equation}
\frac{1}{q_n} \approx e^{H(X_n)}	
\end{equation}

which has the advantage of being 
parametrisation invariant i.e. not depending on the choice of base for logarithms. This generally allows us to simplify calculations. Moreover, this is a robust measure for the expected number of observations that we need to collapse the entropy $H(X_n)$. Equivalently, it is a robust measure of the average distance between rare events. 

On the other hand, the \textit{typical probability} $q_n$ corresponds to the 
density of rare events at the instant $n \in \mathbb{N}$. It may also be understood as the frequency with which we collapse the entropy $H(X_n)$ at the instant $n \in \mathbb{N}$. We may also observe that the typical probability $q_n$ and entropy rate $\frac{1}{q_n}$ are both parametrisation invariant as they are multiplicative inverses of each other.
Together they form the basis for the method of level sets which provides us with 
the necessary and sufficient apparatus for classifying sequences of rare events 
in terms of their rate of entropy production.  

\subsection{A remark on applications of the level set method}

In practice, the empirical data will consist of a binary sequence of rare event 
observations and the actual tests are generally unknown. These may be modelled 
as latent variables that are a function of discrete time(i.e. the integers) as 
well as hidden variables that provide us with initial conditions. Thus, we shall 
generally assume that empirical binary sequences are the output of a discrete dynamical system.

Assuming that deep neural networks may be used as universal data compressors, we may determine whether the data is finite-state incompressible using a machine-learning driven overfitting test provided in the appendix. As a concrete demonstration of the analytical power of the level set method, we shall consider its application to an open problem in probabilistic number theory. 

However, we must first carefully go over the information-theoretic derivation of the Prime Number Theorem [9]. 

\newpage 

\section{Information-theoretic derivation of the Prime Number Theorem}

If we know nothing about the distribution of primes, in the worst case we 
may assume that each prime less than or equal to $N$ is drawn uniformly from $[1,N]$. So our source of primes is: 

\begin{equation}
X \sim U([1,N])	
\end{equation}

where $H(X) = \ln N$ is the Shannon entropy of the uniform distribution. 

Now, we may define the prime encoding of $[1,N]$ as the binary sequence $X_N = \{x_n\}_{n=1}^N$ where $x_n = 1$ if $n$ is prime and $x_n = 0$ otherwise. With no prior knowledge, given that each integer is either prime or not prime, we have $2^N$ possible prime encodings in $[1,N] \subset \mathbb{N}$. 

If there are $\pi(N)$ primes less than or equal to $N$ then the average number of bits per arrangement gives us the average amount of information gained from correctly identifying each prime in $[1,N]$ as: 

\begin{equation}
S_c = \frac{\log_2 (2^N)}{\pi(N)} = \frac{N}{\pi(N)}	
\end{equation}

and if we assume a maximum entropy distribution over the primes then we would expect that each prime is drawn from a uniform distribution as in (19). So we would have: 

\begin{equation}
S_c = \frac{N}{\pi(N)} \sim \ln N	
\end{equation}

As for why the natural logarithm appears in (21), we may first note that the base of the logarithm in the Shannon Entropy may be freely chosen without changing its properties. Moreover, given the assumptions the expected information gained from observing each prime in $[1,N]$ is on the order of: 

\begin{equation}
\sum_{k=1}^{N-1} \frac{1}{k} \cdot |(k,k+1]| = \sum_{k=1}^{N-1} \frac{1}{k} \approx \ln N
\end{equation}

as there are $k$ distinct ways to sample uniformly from $[1,k]$ and a frequency of $\frac{1}{k}$ associated with the event that $k \in \mathbb{P}$. 

This implies that the average number of bits per prime number is given by $\frac{N}{\pi(N)} \sim \ln N$. Rearranging, we find: 

\begin{equation}
\frac{\pi(N)}{N} \sim \frac{1}{\ln N}	
\end{equation}

which is in complete agreement with the Prime Number Theorem. Moreover, this derivation indicates that the prime numbers are empirically distributed as if they were arranged uniformly.  

\subsection{The Shannon source coding theorem and the algorithmic randomness of prime encodings}

By the Shannon source coding theorem, we may infer that $\pi(N)$ primes can't be compressed into fewer than $\pi(N) \cdot \ln N$ bits. Furthermore, as the expected Kolmogorov Complexity equals the Shannon entropy for computable probability distributions: 

\begin{equation}
\mathbb{E}[K(X_N)] \sim 	\pi(N) \cdot \ln N \sim N
\end{equation}

where the identification of $\mathbb{E}[K(X_N)]$ with $\pi(N) \cdot \ln N$ is a direct consequence of the fact that $K(X_N)$ measures the information gained from observing $X_N$ and $\pi(N) \cdot \ln N$ measures the expected information gained from observing $X_N$. From an information-theoretic perspective, this implies that prime encodings are finite-state incompressible as they have a maximum entropy distribution.

The only implicit assumption in this derivation is that all the information in the Universe is conserved as it is only in such Universes that Occam's razor is generally applicable. The Law of Conservation of information which dates back to Von Neumann essentially states that the Von Neumann entropy is invariant to Unitary transformations. 

\section{Application to Cram√©r's random model, Part I}

Using the method of level sets, we may demonstrate that the typical probability that an
integer in the interval $[1,n] \subset \mathbb{N}$ is prime is given by:

\begin{equation}
\forall z \sim U([1,n]), P(z \in \mathbb{P}) \sim \frac{1}{\ln n}
\end{equation}

If $X_n = \{x_i\}_{i=1}^n \in \{0,1\}^n$ defines a prime encoding i.e. a sequence where $x_k = 1$ if $k \in \mathbb{P}$
and $x_k = 0$ otherwise, then we may define $\pi(\sqrt{n})$ primality tests as any composite integer in $[1,n]$
has at most $\pi(\sqrt{n})$ distinct prime factors:

\begin{equation}
\forall A_p \in \{A_{p_k}\}_{k=1}^{\pi(\sqrt{n})}, A_p = \{z \in [1,n]: \text{gcd}(p,z) = 1\} \bigcup \{p\}
\end{equation}

and we'll note that $\forall z \sim U([1,n]), P(z \in A_{p \geq \sqrt{n}}) = 1- \frac{1}{n} + \frac{1}{n}=1$ so we have: 

\begin{equation}
\forall z \sim U([1,n]), H(X_n) = -\ln \prod_{k=1}^{\pi(n)} P(z \in A_{p_k})
 = -\ln \prod_{k=1}^{\pi(\sqrt{n})} P(z \in A_{p_k})
\end{equation}

In order to define $P(z \in A_{p_k})$ we shall implicitly use the approximation that for $p \leq \sqrt{n}, \frac{1}{p}-\frac{1}{n} \approx \frac{1}{p}$ so we have:

\begin{equation}
\forall z \sim U([1,n]), P(z \in A_{p_k}) = \big(1-\frac{1}{p_k}\big) + \frac{1}{n} \approx \big(1-\frac{1}{p_k}\big)
\end{equation}

which allows us to make (27) precise:

\begin{equation}
H(X_n) = -\ln \prod_{p \leq \sqrt{n}} \big(1-\frac{1}{p}\big)
\end{equation}

Using Mertens' third theorem we have:

\begin{equation}
\prod_{p \leq \sqrt{n}} \big(1-\frac{1}{p}\big) \approx \frac{e^{-\gamma}}{\frac{1}{2} \cdot \ln n} \approx \frac{0.9}{\ln n}
\end{equation}

so we may infer that:

\begin{equation}
H(X_n) \sim \ln \ln n
\end{equation}

and therefore $X_n \in \mathcal{L}_{q_n}$ where:

\begin{equation}
\mathcal{L}_{q_n} = \{X_n \in \{0,1\}^{\infty}: H(X_n) \sim \ln\ln n \}
\end{equation}

From (32), we may deduce (25):

\begin{equation}
\forall z \sim U([1,n]), q_n = P(z \in \mathbb{P}) \sim \frac{1}{\ln n}
\end{equation}

which means that the density of the primes at $x \in \mathbb{N}$ is on the order 
of $\sim \frac{1}{\ln x}$ and therefore the expected number of primes less than $n$ is given by: 

\begin{equation}
\pi(n) \sim \int_{2}^n \frac{1}{\ln x} dx \sim \frac{n}{\ln n}
\end{equation}

in complete agreement with the Prime Number Theorem. 

\newpage 

\section{Application to Cram√©r's random model, Part II}

Given the prime encoding $X_n$, let's define the nth prime gap $G_n=p_{n+1}-p_n$ so we may consider the cumulative probability:

\begin{equation}
\sum_{k=1}^{G_n} P(x_{p_n + k} = 1 \land p_{n+1}-p_n = k) = 1
\end{equation}

where $G_n < p_n$ due to Bertrand's postulate and $P(p_{n+1}-p_n=k) = 1$ if and only if we simultaneously measure the values of both $p_n$ and $p_{n+1}$.

As the subsequence $\{x_{p_{n+k}}\}_{k=1}^{G_n}$ halts at $k \in [1,G_n]$ where $x_{p_n+k}=1$, there are at most $G_n$ possible
ways for this sequence to halt. Furthermore, in order to bound the \textit{halting probability} $P(x_{p_n + k} = 1 \land p_{n+1}-p_n = k)$ we may use the density formula (34) to consider its components:

\begin{equation}
\forall k \sim U([1,G_n]), P(p_{n+1} = p_n + k) = P(p_n = p_{n+1} -k) \sim \frac{1}{\ln p_n}
\end{equation}

\begin{equation}
\forall k \sim U([1,G_n]),P(x_{p_n + k} = 1) \sim \frac{1}{\ln p_n}
\end{equation}

where

\begin{equation}
P(x_{p_n + k} = 1 \land p_{n+1}-p_n = k) \geq P(x_{p_n + k} = 1) \cdot P(p_{n+1} = p_n + k)
\end{equation}

Using (36),(37) and (38) we may derive the lower bound:

\begin{equation}
\forall k \sim U([1,G_n]),P(x_{p_n + k} = 1 \land p_{n+1}-p_n = k) \gsim \frac{1}{(\ln p_n)^2}
\end{equation}

which implies:

\begin{equation}
\frac{1}{(\ln p_n)^2} \lsim \frac{1}{G_n} \sum_{k=1}^{G_n} P(x_{p_n + k} = 1 \land p_{n+1}-p_n = k) = \frac{1}{G_n}
\end{equation}

and since $G_n = p_{n+1}-p_n$, we may conclude:

\begin{equation}
p_{n+1}-p_n = \mathcal{O}((\ln p_n)^2)
\end{equation}

as conjectured by Harald Cram√©r in 1936 [1].

\newpage

\section{Details of the $\frac{1}{p} - \frac{1}{n} \approx \frac{1}{p}$ approximation in Cram√©r's model}

If we define,

\begin{equation}
\Lambda_k := \text{set of } {\pi(\sqrt{n}) \choose k} \text{ distinct subsets of } \{p_k\}_{k=1}^{\pi(\sqrt{n})}
\end{equation}

then $\lvert \Lambda_k \rvert = {\pi(\sqrt{n}) \choose k}$ and we may derive the absolute error:

\begin{equation}
\Big\lvert \prod_{p \leq \sqrt{n}} \big(1-\frac{1}{p} + \frac{1}{n}) - \prod_{p \leq \sqrt{n}} \big(1-\frac{1}{p}\big) \Big\rvert \leq \sum_{k=1}^{\pi(\sqrt{n})-1} \frac{1}{n^k} \sum_{\lambda \in \Lambda_k} \prod_{p \in \lambda} \big(1-\frac{1}{p}\big) + \frac{1}{n^{\pi(\sqrt{n})}}
\end{equation}

and given that:

\begin{equation}
\pi(\sqrt{n}) < \sqrt{n} \implies \frac{1}{n^k} \cdot {\pi(\sqrt{n}) \choose k} \leq \frac{1}{n^{k/2}}
\end{equation}

the absolute error satisfies the following inequality:

\begin{equation}
\Delta_n = \Big\lvert \prod_{p \leq \sqrt{n}} \big(1- \frac{1}{p}+\frac{1}{n}\big) - \prod_{p \leq \sqrt{n}} \big(1- \frac{1}{p}\big)  \Big\rvert \leq \frac{1}{\sqrt{n}} + \frac{1}{n}
\end{equation}

so the relative error converges to zero:

\begin{equation}
\lim_{n \to \infty} \frac{\Delta_n}{\prod_{p \leq \sqrt{n}} \big(1- \frac{1}{p}\big)} \leq \lim_{n \to \infty} \big(\frac{\ln n}{\sqrt{n}} + \frac{\ln n}{n}\big) = 0
\end{equation}

which provides us with the necessary justifications in our derivation of Cram√©r's random model, specifically
formulas (29),(30) and (31).

\newpage 

\section{Conclusion}

The finite-state incompressibility of prime encodings has one important implication which has so far been ignored by the scientific community. 

As humans are finite-state machines, humans can't have developed¬†the notion of prime numbers(i.e. the Unique Factorization Theorem) via inductive inference. The only other possibility, which¬†is consistent with¬†the history of Greek mathematics, is that this notion emerged via deductive¬†reasoning.¬†

\section*{References}

\small

[1] Cram√©r, H. "On the Order of Magnitude of the Difference Between Consecutive Prime Numbers." Acta Arith. 2, 23-46, 1936.

[2] Hardy, G. H.; Ramanujan, S. (1917), "The normal number of prime factors of a number n", Quarterly Journal of Mathematics

[3] Tur√°n, P√°l (1934), "On a theorem of Hardy and Ramanujan", Journal of the London Mathematical Society

[4] Yufei Zhao. The Probabilistic Method in Combinatorics. 2019.

[5] F. Mertens. J. reine angew. Math. 78 (1874)

[6] Olivier Rioul. This is IT: A Primer on Shannon‚Äôs Entropy and Information. S√©minaire Poincar√©. 2018.

[7] E.T. Jaynes. Information Theory and Statistical Mechanics. The Physical Review. 1957.

[8] Lance Fortnow. Kolmogorov Complexity. 2000.

[9] Aidan Rocke (https://mathoverflow.net/users/56328/aidan-rocke), information-theoretic derivation of the prime number theorem, URL (version: 2021-04-08): https://mathoverflow.net/q/384109

[10] Aidan Rocke (https://mathoverflow.net/users/56328/aidan-rocke), Egyptian number theory, URL (version: 2021-06-22): https://mathoverflow.net/q/395939

\newpage 

\appendix

\section{An invariance theorem for algorithmically random data}

Let's suppose we have a natural signal described by the process $X$: 

\begin{equation}
x_n \in \{0,1\}, x_{n+1} = \varphi \circ x_{1:n}	
\end{equation}

If we should use machine learning to approximate $\varphi$ given the datasets 
$X_N^{\text{train}}= \{x_i\}_{i=1}^N,X_N^{\text{test}}= \{x_i\}_{i=N+1}^{2N}$
such that for any $\hat{f} \in F_{\theta}$: 

\begin{equation}
\exists k \in [1,n-1], x_{n+1} = \hat{f} \circ x_{n-k:n} \Rightarrow \delta_{\hat{f}(x_{n-k:n}),x_{n+1}} = 1	
\end{equation}

then $X_N$ is asymptotically incompressible if for large $N$ any solution to the 
empirical risk minimisation problem: 

\begin{equation}
\hat{f} = \max_{f \in F_{\theta}} \frac{1}{N-k} \sum_{n=k+1}^N \delta_{f(x_{n-k:n}),x_{n+1}}
\end{equation}

has an expected performance: 

\begin{equation}
\frac{1}{N-k} \sum_{n=N+k+1}^{2N-1} \delta_{\hat{f}(x_{n-k:n}),x_{n+1}}
 \leq \frac{1}{2}
\end{equation}

Furthermore, if the dataset is imbalanced i.e. $\frac{1}{N} \sum_{i=1}^N x_i \neq \frac{1}{2}$ then we may generalise this result by introducing the auxiliary definitions: 

\begin{equation}
y_n = x_{n+1}	
\end{equation}

\begin{equation}
\hat{y_n} = \hat{f} \circ x_{n-k:n}
\end{equation}

\begin{equation}
\beta_n = \delta_{y_n,\hat{y_n}}	
\end{equation}

\begin{equation}
N_1 = \sum_{n=N+k+1}^{2N} \delta_{y_n,1}	
\end{equation}

\begin{equation}
N_0 = \sum_{n=N+k+1}^{2N} \delta_{y_n,0}	
\end{equation}

and so for large $N$, we have: 

\begin{equation}
\mathcal{L}_N[\hat{f}] = \min \Big[\frac{1}{N_0} \sum_{n=N+k+1}^{2N-1} \delta_{y_n,0} \cdot \beta_n,  \frac{1}{N_1} \sum_{n=N+k+1}^{2N-1} \delta_{y_n,1} \cdot \beta_n \Big] \leq \frac{1}{2}
\end{equation}

and therefore:


\begin{equation}
\forall \hat{f} \in F_{\theta}, \lim_{N \to \infty} P(\mathcal{L}_N[\hat{f}] > \frac{1}{2}) = 0
\end{equation}

Finally, as these results are invariant to transformations that preserve the 
phase-space dimension of $X$, this theorem may be used as an overfitting test 
for binary sequences that are finite-state incompressible. 


\end{document}