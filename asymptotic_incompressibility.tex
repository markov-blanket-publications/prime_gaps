\documentclass{article}

% ready for submission
\usepackage{arxiv}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\title{Asymptotic incompressibility and its application to rare-event modelling}

\date{April 8, 2021}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Aidan Rocke\\
  \texttt{aidanrocke@gmail.com} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
   The objective of this article is to present a theory of algorithmic information that allows us to characterise rare events that are recurrent, of variable frequency, and unpredictable. Within this theory, it may be shown that such data-generating processes are asymptotically incompressible and therefore identifiable via an invariance theorem for 
   algorithmically random data. While such phenomena define the worst case in rare-event modelling, a robust approach for distinguishing such cases which are intractable from cases which are algorithmically tractable has so far been lacking. 
   \end{abstract}

\section{Unpredictable phenomena are asymptotically incompressible}

Given a rare-event process $X$, a scientist may only collect a finite number of observations $X_N = \{x_i\}_{i=1}^N$ from that process. Moreover, let's suppose $x_i = 1$ when the event of interest is observed and $x_i=0$ otherwise. Then the location of the rare events, i.e. $i$ where $x_i=1$, are necessary and sufficient to define the binary encoding $X_N$.

This process is deterministic if there exists a computable function $f$ such that: 

\begin{equation}
x_{n+1} = f \circ x_{1:n}	
\end{equation}

and inductive generalisation is algorithmically feasible if there exists $k < \infty$ such that: 

\begin{equation}
x_{n+1} = f \circ x_{n-k:n}	
\end{equation}

and if there exists an asymptotic formula $\pi(N)$ such that for large $N$, 

\begin{equation}
\pi(N) \sim \sum_{i=1}^N x_i	
\end{equation}

then we say that $X$ is an unpredictable process if the average amount of information gained from the occurrence of each rare-event is given by the combinatorial entropy: 

\begin{equation}
S_c = \frac{\log_2 (2^N)}{\pi(N)} = \frac{N}{\pi(N)} \sim \ln(N)
\end{equation}

where $2^N$ is exactly the number of possible binary encodings of length $N$ and 
$\sim \ln(N)$ implies that the rate of entropy production is maximal. (3) also implies that $X_N$ may not be compressed into fewer than $\pi(N) \cdot \ln(N)$ bits without being certain that information will be lost. 

\newpage

In fact, $X_N$ is asymptotically incompressible in the sense that: 

\begin{equation}
\mathbb{E}[K(X_N)] \sim 	\pi(N) \cdot \ln(N) \sim N
\end{equation}

as (3) states that on average each observation $x_i$ is a surprising event.  

Now, a direct implication of (4) is that any approximation $\hat{f} \in F_{\theta}$ of $f$ discovered using machine learning or another scientific 
method such that the Kronecker delta satisfies: 

\begin{equation}
\forall n \in [1,N], \delta_{\hat{f}(x_{n-k:n}),x_{n+1}} = 1	
\end{equation}

has an algorithmic complexity that scales as follows: 

\begin{equation}
K(\hat{f}) \sim N	
\end{equation}

as such high levels of accuracy are due to memorisation and not discovering 
regularities in $X_N$. Given (4), (5), and (6) we may state that the process $X$ is algorithmically 
random with respect to $F_{\theta}$. 

\section{A derivation of the maximum entropy distribution}

Let's define the sequence $\mathbb{P} = \{p_k\}_{k=1}^\infty \subset \mathbb{N}$
such that $p_k \in \mathbb{P}$ if and only if $x_{p_k} = 1$. Then the $\ln(N)$ term in 
(4) implies that the elements of $\mathbb{P}$ are in some sense uniformly distributed in $X_N$. 

Given that there are $k$ distinct ways to sample uniformly from $[1,k]$ and a frequency of $\frac{1}{k}$ associated with the event $(k-1,k] \cap \mathbb{P} \neq \emptyset$ the average entropy rate has a natural interpretation. 

By breaking $\sum_{k=1}^N \frac{1}{k}$ into $\pi(N)$ disjoint blocks of size $[p_k,p_{k+1}]$ where $p_k, p_{k+1} \in \mathbb{P}$: 

\begin{equation}
\sum_{k=1}^N \frac{1}{k}	 \approx \sum_{k=1}^{\pi(N)} \sum_{n=p_k}^{p_{k+1}} \frac{1}{n} = \sum_{k=1}^{\pi(N)} (p_{k+1}-p_k) \cdot P(p_k) \approx \ln(N)
\end{equation}

where $P(p_k)= \frac{1}{p_{k+1}-p_k}\sum_{n=p_k}^{p_{k+1}} \frac{1}{n}$. 

So we see that (4) approximates the expected number of observations per rare event where $P(p_k)$ may be interpreted as the probability of a successful observation in a frequentist sense. This is consistent with John Wheeler's \textit{it from bit} interpretation of entropy where entropy measures the average number of bits(i.e. yes/no questions) per rare event. 

Interestingly, (7) may also be interpreted as the expected distance or waiting time between consecutive rare events as we have: 

\begin{equation}
\mathbb{E}[|p_{n+1} -p_n|] = \sum_{k=1}^{\pi(N)} (p_{k+1}-p_k) \cdot P(p_k) \approx \ln(N) 	
\end{equation}

Having clarified the rate of entropy production of $X$ we may consider a practical method for identifying data-generating processes that are asymptotically incompressible. 

\newpage

\section{An invariance theorem for algorithmically random data}

Let's suppose we have a natural signal described by the process $X$: 

\begin{equation}
x_n \in \{0,1\}, x_{n+1} = \varphi \circ x_{1:n}	
\end{equation}

If we should use machine learning to approximate $\varphi$ given the datasets 
$X_N^{\text{train}}= \{x_i\}_{i=1}^N,X_N^{\text{test}}= \{x_i\}_{i=N+1}^{2N}$
such that for any $\hat{f} \in F_{\theta}$: 

\begin{equation}
\exists k \in [1,n-1], x_{n+1} = \hat{f} \circ x_{n-k:n} \Rightarrow \delta_{\hat{f}(x_{n-k:n}),x_{n+1}} = 1	
\end{equation}

then $X_N$ is asymptotically incompressible if for large $N$ any solution to the 
empirical risk minimisation problem: 

\begin{equation}
\hat{f} = \max_{f \in F_{\theta}} \frac{1}{N-k} \sum_{n=k+1}^N \delta_{f(x_{n-k:n}),x_{n+1}}
\end{equation}

has an expected performance: 

\begin{equation}
\frac{1}{N-k} \sum_{n=N+k+1}^{2N-1} \delta_{\hat{f}(x_{n-k:n}),x_{n+1}}
 \leq \frac{1}{2}
\end{equation}

Furthermore, if the dataset is imbalanced i.e. $\frac{1}{N} \sum_{i=1}^N x_i \neq \frac{1}{2}$ then we may generalise this result by introducing the auxiliary definitions: 

\begin{equation}
y_n = x_{n+1}	
\end{equation}

\begin{equation}
\hat{y_n} = \hat{f} \circ x_{n-k:n}
\end{equation}

\begin{equation}
\beta_n = \delta_{y_n,\hat{y_n}}	
\end{equation}

\begin{equation}
N_1 = \sum_{n=N+k+1}^{2N} \delta_{y_n,1}	
\end{equation}

\begin{equation}
N_0 = \sum_{n=N+k+1}^{2N} \delta_{y_n,0}	
\end{equation}

and so for large $N$, we have: 

\begin{equation}
\mathcal{L}_N[\hat{f}] = \min \Big[\frac{1}{N_0} \sum_{n=N+k+1}^{2N-1} \delta_{y_n,0} \cdot \beta_n,  \frac{1}{N_1} \sum_{n=N+k+1}^{2N-1} \delta_{y_n,1} \cdot \beta_n \Big] \leq \frac{1}{2}
\end{equation}

and therefore:


\begin{equation}
\forall \hat{f} \in F_{\theta}, \lim_{N \to \infty} P(\mathcal{L}_N[\hat{f}] > \frac{1}{2}) = 0
\end{equation}

Finally, as these results are invariant to transformations that preserve the 
phase-space dimension of $X$, this theorem may be used as an overfitting test 
for algorithmically-random data. 

\newpage

\section{Discussion}

A useful interpretation of (19) is that the sample-complexity of an approximately correct prime formula is infinite and therefore it is not learnable. Moreover, given that the prime numbers are the only known rare-event process with a distribution satisfying (4): 

\begin{equation}
\pi(N) \sim \frac{N}{\ln(N)}	
\end{equation}

the overfitting test (19) may be applied to the scientific problem of identifying cosmic signals communicated by civilisations within the Turing limit that are 
sufficiently advanced to do number theory. 

\section*{References}

\small

[1] John A. Wheeler, 1990, "Information, physics, quantum: The search for links" in W. Zurek (ed.) Complexity, Entropy, and the Physics of Information. Redwood City, CA: Addison-Wesley.

[2] Doron Zagier. Newman’s short proof of the Prime Number Theorem. The American Mathematical Monthly, Vol. 104, No. 8 (Oct., 1997), pp. 705-708

[3] M. Li and P. Vitányi. An Introduction to Kolmogorov Complexity and Its Applications. Graduate Texts in Computer Science. Springer. 1997.

[4] Peter Shor. Shannon’s noiseless coding theorem. lecture notes. 2010.

[5] Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie. The Elements of Statistical Learning. Springer. 2001.

[6] Andrew Chi-Chih Yao. Theory and applications of trapdoor functions. In Proceedings of the 23rd IEEE Symposium on Foundations of Computer Science, 1982.

[7] Sullivan, Walter (September 29, 1968). “The Universe Is Not Ours Alone; From the edge of the galaxy”. The New York Times.

[8] Alexander L. Zaitsev. Rationale for METI. Arxiv. 2011.

\end{document}